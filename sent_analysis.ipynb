{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf11c7-5b38-4898-8150-8a7b3fb31304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import string\n",
    "import contractions\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67241d-e46b-4645-bca5-6c158c5fab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "\n",
    "# Drop rows with missing 'Score' values\n",
    "df = df.dropna(subset=['Score'])\n",
    "\n",
    "# Filter only positive (4, 5) and negative (1, 2) reviews, excluding neutral reviews (3 stars)\n",
    "filtered_data = df[df['Score'].isin([1, 2, 4, 5])]\n",
    "\n",
    "# Drop duplicates to ensure unique Product IDs\n",
    "unique_products = filtered_data.drop_duplicates(subset=['ProductId'])\n",
    "\n",
    "# Randomly sample 20,000 rows from the filtered dataset\n",
    "sampled_data = unique_products.sample(n=20000, random_state=42)\n",
    "\n",
    "# Save the sampled dataset to a new CSV file (optional)\n",
    "sampled_data.to_csv('filtered_reviews_20000.csv', index=False)\n",
    "\n",
    "print(f\"Dataset created with {len(sampled_data)} reviews from unique products.\")\n",
    "print(sampled_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d70404-c28c-47ff-bb0a-dff7327c3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count negative reviews (Score = 1 or 2)\n",
    "negative_count = sampled_data[sampled_data['Score'].isin([1, 2])].shape[0]\n",
    "print(f\"Number of negative reviews: {negative_count}\")\n",
    "\n",
    "# Count positive reviews (Score = 4 or 5)\n",
    "positive_count = sampled_data[sampled_data['Score'].isin([4, 5])].shape[0]\n",
    "print(f\"Number of positive reviews: {positive_count}\")\n",
    "\n",
    "# Verify the total number of rows\n",
    "total_reviews = sampled_data.shape[0]\n",
    "print(f\"Total reviews in the dataset: {total_reviews}\")\n",
    "print(f\"Sum of positive and negative reviews: {negative_count + positive_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f92dff-db3a-4213-a1ae-d2c164605a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sampled dataset to the project directory (e.g., 'data/' folder)\n",
    "sampled_data.to_csv('filtered_reviews_20000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8003d91f-6f9e-4647-8ac8-25442ede4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the required structure\n",
    "updated_data = sampled_data[['Text', 'Score']].copy()\n",
    "\n",
    "# Add a new 'id' column with incremental values\n",
    "updated_data.insert(0, 'id', range(1, len(updated_data) + 1))\n",
    "\n",
    "# Convert scores: 4 and 5 to 1, 1 and 2 to 0\n",
    "updated_data['Score'] = updated_data['Score'].map({4: 1, 5: 1, 1: 0, 2: 0})\n",
    "\n",
    "# Rename the 'Text' column to 'comments'\n",
    "updated_data.rename(columns={'Text': 'Reviews'}, inplace=True)\n",
    "\n",
    "# Save the updated dataset to a new file (optional)\n",
    "updated_data.to_csv('updated_reviews.csv', index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "print(updated_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef615db-1f73-4331-a7b1-c15af93a6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataset\n",
    "preprocessed_data = updated_data.copy()\n",
    "\n",
    "# Define a spelling correction mapping dictionary\n",
    "spelling_correction_mapping = contraction_mapping = {\n",
    "    \"ive\": \"i've\", \"dont\": \"don't\", \"cant\": \"can't\", \"wont\": \"won't\", \"im\": \"i'm\", \"youre\": \"you're\",\n",
    "    \"theyre\": \"they're\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wasnt\": \"wasn't\", \"werent\": \"weren't\",\n",
    "    \"havent\": \"haven't\", \"hasnt\": \"hasn't\", \"hadnt\": \"hadn't\", \"wouldnt\": \"wouldn't\", \"doesnt\": \"doesn't\",\n",
    "    \"didnt\": \"didn't\", \"couldnt\": \"couldn't\", \"shouldnt\": \"shouldn't\", \"mightnt\": \"mightn't\", \"mustnt\": \"mustn't\",\n",
    "    \"whos\": \"who's\", \"whats\": \"what's\", \"wheres\": \"where's\", \"whens\": \"when's\", \"hows\": \"how's\", \"ill\": \"i'll\",\n",
    "    \"youll\": \"you'll\", \"hell\": \"he'll\", \"theyll\": \"they'll\", \"itll\": \"it'll\", \"thatll\": \"that'll\", \"youd\": \"you'd\",\n",
    "    \"hed\": \"he'd\", \"theyd\": \"they'd\", \"thatd\": \"that'd\", \"youve\": \"you've\", \"weve\": \"we've\", \"theyve\": \"they've\",\n",
    "    \"shouldve\": \"should've\", \"couldve\": \"could've\", \"lets\": \"let's\", \"aint\": \"ain't\"\n",
    "}\n",
    "\n",
    "\n",
    "# Define a combined preprocessing function\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Step 1: Text Normalization\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Step 2: Correct Spelling Errors\n",
    "    words = text.split()\n",
    "    corrected_words = [spelling_correction_mapping.get(word, word) for word in words]\n",
    "    text = \" \".join(corrected_words)\n",
    "    \n",
    "    # Step 3: Expand Contractions using the contractions library\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Step 4: Remove Emails\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Step 5: Remove HTML Tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Step 6: Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Step 7: Handle Accented Characters\n",
    "    text = unidecode.unidecode(text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing function to overwrite the Reviews column\n",
    "preprocessed_data['Reviews'] = preprocessed_data['Reviews'].apply(preprocess_text)\n",
    "\n",
    "# Save the final preprocessed dataset\n",
    "preprocessed_data.to_csv('preprocessed_data.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing completed. Dataset saved as 'preprocessed_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8565c3bf-cbfc-4321-a85d-e9762a43fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = preprocessed_data['Score'].values.tolist()\n",
    "review = preprocessed_data['Reviews'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ef9bb-d3e7-4ee8-9953-3ba256d4c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "portion = int(len(rating)*0.8)\n",
    "\n",
    "review_train = review[:portion]\n",
    "review_test = review[portion:]\n",
    "rating_train = rating[:portion]\n",
    "rating_test = rating[portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "df7242c6-9b42-4c94-911c-aeab56bd241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words covering 95% of the dataset: 4792\n",
      "The top 4500 words cover 94.69% of the total tokens.\n"
     ]
    }
   ],
   "source": [
    "# Sample reviews\n",
    "review2 = preprocessed_data['Reviews'].tolist()\n",
    "\n",
    "# Tokenizer without num_words to analyze coverage\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(review2)\n",
    "\n",
    "# Analyze word coverage\n",
    "word_counts = tokenizer.word_counts\n",
    "sorted_counts = sorted(word_counts.values(), reverse=True)\n",
    "cumulative_coverage = np.cumsum(sorted_counts) / sum(sorted_counts)\n",
    "\n",
    "# Determine the number of words covering 95% of the dataset\n",
    "num_words_95 = np.argmax(cumulative_coverage >= 0.95) + 1\n",
    "print(f\"Number of words covering 95% of the dataset: {num_words_95}\")\n",
    "\n",
    "# Calculate cumulative coverage\n",
    "total_tokens = sum(sorted_counts)  # Total number of tokens in the dataset\n",
    "top_4500_coverage = sum(sorted_counts[:4500]) / total_tokens * 100\n",
    "\n",
    "print(f\"The top 4500 words cover {top_4500_coverage:.2f}% of the total tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "23036da5-64f3-4fc7-b72c-1679a18f6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data for training\n",
    "num_words = 5000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "1312e571-cd2c-406c-9f4b-0e6ba8621882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34458"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "990f3254-eb04-43f3-8656-4a66f8b66f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data for training\n",
    "review_train_tokens = tokenizer.texts_to_sequences(review_train)\n",
    "review_test_tokens = tokenizer.texts_to_sequences(review_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "a4cd91f3-5dd0-4a17-a075-94ffc313afef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1988"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_train[1699].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b7554e18-50e1-438d-b870-1b30bf8b8aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_train_tokens[1699])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "4dc64a06-3d20-4b50-aaf6-726a7018e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in review_train_tokens + review_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "e874c89e-58bd-4c74-9fad-62097cfa3bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.52325"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "6d3fb0ea-1ecc-45ab-9b35-4fb41a7afec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "ee026e45-03d0-4968-be7e-d980eb32b957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1699"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e7a543ff-957a-4bcb-a457-20b331e8b66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "e10eb348-e9b1-49ae-af69-8c5cc160aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96185"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "d69acfad-8e24-4997-85e9-b218478da643",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_train_pad = pad_sequences(review_train_tokens, maxlen=max_tokens)\n",
    "review_test_pad = pad_sequences(review_test_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "541b8b80-4cb7-4c3c-93e6-07bd37a5b499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  259,   47,  596,   26,    6,  475,   27,   19,   96,   11,\n",
       "       2666,   65,   27,   24,   47,   17,   13,    5,    7,   81,    6,\n",
       "        655,   18,    1, 2010])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(review_train_tokens[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7f028-847e-4cf7-a458-39cc653eb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_train_pad[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca0f38-1db6-4537-97df-5a7fb0fcea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102af254-73ca-4772-89fc-86993a841519",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "175090cd-75e9-417a-aa1e-ad6fcde1bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data for training\n",
    "index = tokenizer.word_index\n",
    "index_word = dict([(value, key) for (key, value) in index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "031ba041-af2f-42fe-9f5c-6d536033d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert tokenized text back to human-readable text\n",
    "def decode_review(tokens):\n",
    "    return ' '.join([index_word.get(i, '?') for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "b4e01b55-7ad2-4f93-8527-ec4a187ad084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the people we sent these to liked them but did not rave about them. so we are concluding that it is best to stick with the cashews'"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_train[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "7aa68087-9d1e-4b68-bd0f-7e705f185a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the people we sent these to liked them but did not rave about them so we are that it is best to stick with the cashews'"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to convert tokenized text back to human-readable text\n",
    "decode_review(review_train_tokens[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6e6ad92e-a7c3-4b48-987f-f8296e1df6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings into a dictionary\n",
    "glove_embeddings = {}\n",
    "with open('glove.6B/glove.6B.100d.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype=\"float32\")\n",
    "        glove_embeddings[word] = vector\n",
    "print(\"GloVe embeddings loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "c8f60f21-f0f4-4d46-b403-727fb0613963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 5000\n",
      "Missing words in the first 5000: 72\n",
      "Missing words examples: [\"amazon's\", \"bob's\", \"dog's\", \"newman's\", \"joe's\", 'eacute', \"nature's\", \"sam's\", \"company's\", \"earth's\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "glove_vocab = set(glove_embeddings.keys())\n",
    "\n",
    "# num_words sınırına göre ilk kelimeleri kontrol et\n",
    "missing_words = [\n",
    "    word for word, index in tokenizer.word_index.items()\n",
    "    if index < num_words and word not in glove_vocab\n",
    "]\n",
    "\n",
    "# Eksik kelimeleri görüntüle\n",
    "print(f\"Total words in vocab: {num_words}\")\n",
    "print(f\"Missing words in the first {num_words}: {len(missing_words)}\")\n",
    "print(\"Missing words examples:\", missing_words[:10])  # İlk 10 eksik kelime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "8d3770ac-5487-4bdf-94cf-b2df673ee37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding matrix\n",
    "embedding_dim = 100\n",
    "vocab_size = num_words + 1  # `word_index` should be defined earlier during tokenization\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# UNK token için rastgele embedding başlatma\n",
    "unk_embedding = np.random.uniform(-0.25, 0.25, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "33407ce0-5355-4188-8398-48987cb19045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5001"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "c3eae68d-ac6c-4faf-8192-afbeeb1473a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding matrix...\n",
      "Embedding matrix created! Missing words: 72\n"
     ]
    }
   ],
   "source": [
    "# Fill the embedding matrix\n",
    "print(\"Creating embedding matrix...\")\n",
    "missing_words = 0\n",
    "for word, idx in tokenizer.word_index.items():  # use word_index from tokenizer\n",
    "    if idx >= vocab_size:\n",
    "        continue\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector  # Words found in GloVe\n",
    "    else:\n",
    "        embedding_matrix[idx] = unk_embedding  # Words not found in GloVe, use UNK embedding\n",
    "        missing_words += 1  # Words not found in GloVe\n",
    "\n",
    "print(f\"Embedding matrix created! Missing words: {missing_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "af52bb27-bd9d-4fb4-b2ee-2425aa496c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 100)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a9211bda-e336-4f58-82de-85ba8aff6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "4b6fc1f2-baef-4db7-b1c2-5553726a509d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huso/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_25\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_25\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">500,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_25 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │       \u001b[38;5;34m500,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_53 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_54 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">500,100</span> (1.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m500,100\u001b[0m (1.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">500,100</span> (1.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m500,100\u001b[0m (1.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 91ms/step - accuracy: 0.8438 - loss: 0.4047 - val_accuracy: 0.8778 - val_loss: 0.2736\n",
      "Epoch 2/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 87ms/step - accuracy: 0.9100 - loss: 0.2144 - val_accuracy: 0.9091 - val_loss: 0.2202\n",
      "Epoch 3/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 85ms/step - accuracy: 0.9484 - loss: 0.1388 - val_accuracy: 0.9003 - val_loss: 0.2411\n",
      "Epoch 4/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 85ms/step - accuracy: 0.9604 - loss: 0.1096 - val_accuracy: 0.9169 - val_loss: 0.2308\n",
      "Epoch 5/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 85ms/step - accuracy: 0.9734 - loss: 0.0807 - val_accuracy: 0.9109 - val_loss: 0.2696\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer with pre-trained GloVe embeddings\n",
    "model.add(Embedding(\n",
    "    input_dim=vocab_size,  # Vocabulary size (including padding token)\n",
    "    output_dim=100,  # GloVe embedding dimension\n",
    "    weights=[embedding_matrix],  # Pre-trained embedding matrix\n",
    "    input_length=max_tokens,  # Maximum sequence length\n",
    "    trainable=True  # Freeze embeddings\n",
    "))\n",
    "\n",
    "# SpatialDropout1D for regularization\n",
    "#model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# First LSTM layer\n",
    "#model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "\n",
    "# First LSTM layer\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "\n",
    "# Second LSTM layer\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "\n",
    "# Dense layer for further feature extraction\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer for sentiment classification\n",
    "model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "rating_train = np.array(rating_train)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    review_train_pad,\n",
    "    rating_train,\n",
    "    batch_size=16,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "#loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "#print(f\"Test Loss: {loss}\")\n",
    "#print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1c9b1f3d-8cc8-4cac-897c-a81c03f0fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_test = np.array(rating_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "d545bbe4-d840-4605-b84d-55ac1a573b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9313 - loss: 0.2239\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "evaluation_result = model.evaluate(review_test_pad, rating_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "488af54b-fc72-44c3-bcb1-375ec054bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2045\n",
      "Test Accuracy: 0.9352\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(f\"Test Loss: {evaluation_result[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {evaluation_result[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "d22b0ef8-c98c-4ade-a4f0-e6a5713ccb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(review_test_pad, verbose=1)\n",
    "test_predictions = (test_predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "9832f8f8-4d82-4613-a83c-3a8863ca5138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "Accuracy: 0.9353\n",
      "Precision: 0.9513\n",
      "Recall: 0.9744\n",
      "F1-Score: 0.9627\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.70      0.75       569\n",
      "           1       0.95      0.97      0.96      3431\n",
      "\n",
      "    accuracy                           0.94      4000\n",
      "   macro avg       0.89      0.84      0.86      4000\n",
      "weighted avg       0.93      0.94      0.93      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(rating_test, test_predictions)\n",
    "precision = precision_score(rating_test, test_predictions)\n",
    "recall = recall_score(rating_test, test_predictions)\n",
    "f1 = f1_score(rating_test, test_predictions)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(rating_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "bcf470bd-3929-4f09-b789-7574da883c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "Cümle: The product is absolutely amazing, much better than I expected!\n",
      "Tahmin edilen sınıf: Pozitif\n",
      "Model çıktısı (olasılık): 0.9897\n",
      "--------------------------------------------------\n",
      "Cümle: My order arrived very quickly, and the packaging was excellent. Thank you!\n",
      "Tahmin edilen sınıf: Pozitif\n",
      "Model çıktısı (olasılık): 0.9996\n",
      "--------------------------------------------------\n",
      "Cümle: I'm very satisfied with this service, and I will definitely use it again.\n",
      "Tahmin edilen sınıf: Pozitif\n",
      "Model çıktısı (olasılık): 0.9984\n",
      "--------------------------------------------------\n",
      "Cümle: The food was delicious and reasonably priced for its quality.\n",
      "Tahmin edilen sınıf: Pozitif\n",
      "Model çıktısı (olasılık): 0.9982\n",
      "--------------------------------------------------\n",
      "Cümle: The camera on this phone is outstanding, even in low light conditions.\n",
      "Tahmin edilen sınıf: Pozitif\n",
      "Model çıktısı (olasılık): 0.9985\n",
      "--------------------------------------------------\n",
      "Cümle: Unfortunately, the product arrived damaged, and I couldn't use it.\n",
      "Tahmin edilen sınıf: Negatif\n",
      "Model çıktısı (olasılık): 0.3501\n",
      "--------------------------------------------------\n",
      "Cümle: The delivery was extremely late, and customer service was unhelpful.\n",
      "Tahmin edilen sınıf: Negatif\n",
      "Model çıktısı (olasılık): 0.3377\n",
      "--------------------------------------------------\n",
      "Cümle: The quality is very poor, and it's not worth the price at all.\n",
      "Tahmin edilen sınıf: Negatif\n",
      "Model çıktısı (olasılık): 0.1435\n",
      "--------------------------------------------------\n",
      "Cümle: The restaurant was filthy, and the service was very slow.\n",
      "Tahmin edilen sınıf: Pozitif\n",
      "Model çıktısı (olasılık): 0.9574\n",
      "--------------------------------------------------\n",
      "Cümle: The product I received was completely different from what I ordered.\n",
      "Tahmin edilen sınıf: Negatif\n",
      "Model çıktısı (olasılık): 0.1419\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example test sentences in English\n",
    "test_sentences = [\n",
    "    \"The product is absolutely amazing, much better than I expected!\",\n",
    "    \"My order arrived very quickly, and the packaging was excellent. Thank you!\",\n",
    "    \"I'm very satisfied with this service, and I will definitely use it again.\",\n",
    "    \"The food was delicious and reasonably priced for its quality.\",\n",
    "    \"The camera on this phone is outstanding, even in low light conditions.\",\n",
    "    \"Unfortunately, the product arrived damaged, and I couldn't use it.\",\n",
    "    \"The delivery was extremely late, and customer service was unhelpful.\",\n",
    "    \"The quality is very poor, and it's not worth the price at all.\",\n",
    "    \"The restaurant was filthy, and the service was very slow.\",\n",
    "    \"The product I received was completely different from what I ordered.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenize ve pad etme\n",
    "test_tokens = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_tokens, maxlen=max_tokens)\n",
    "\n",
    "# Modelle tahmin yapma\n",
    "predictions = model.predict(test_padded)\n",
    "\n",
    "# Tahmin edilen sınıfları çıkarma (0 veya 1)\n",
    "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Sonuçları yazdırma\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"Cümle: {sentence}\")\n",
    "    print(f\"Tahmin edilen sınıf: {'Pozitif' if predicted_classes[i] == 1 else 'Negatif'}\")\n",
    "    print(f\"Model çıktısı (olasılık): {predictions[i][0]:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d32efd-3a8c-4977-be5f-b7bd568dec56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
